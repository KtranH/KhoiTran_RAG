import os
import gradio as gr
import logging
import time
import requests
from dotenv import load_dotenv
from document_processor import DocumentProcessor
from document_query import DocumentQuery
from database_query import DatabaseQuery
from hybrid_query import HybridQuery

# Load environment variables
load_dotenv()

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ƒê·ªçc c√°c bi·∫øn m√¥i tr∆∞·ªùng
LM_STUDIO_URL = os.getenv("LM_STUDIO_URL", "http://127.0.0.1:1234")
MODEL_NAME = os.getenv("MODEL_NAME", "gemma-3-12b-it")
PERSIST_DIRECTORY = os.getenv("PERSIST_DIRECTORY", "./chroma_db")
MYSQL_HOST = os.getenv("MYSQL_HOST", "localhost")
MYSQL_USER = os.getenv("MYSQL_USER", "root")
MYSQL_PASSWORD = os.getenv("MYSQL_PASSWORD", "")
MYSQL_PORT = int(os.getenv("MYSQL_PORT", "3306"))
MYSQL_DATABASE = os.getenv("MYSQL_DATABASE", "kt_ai")
TOP_K = int(os.getenv("TOP_K", "3"))

# M√†u s·∫Øc v√† CSS t√πy ch·ªânh
DARK_CUSTOM_CSS = """
.container {
    max-width: 1200px;
    margin: auto;
}
.status-connected {
    color: #22c55e;
    font-weight: bold;
}
.status-error {
    color: #ef4444;
    font-weight: bold;
}
.query-info {
    background-color: #1e293b;
    border-left: 4px solid #4f46e5;
    padding: 10px;
    margin: 10px 0;
    border-radius: 4px;
}
.source-info {
    margin-top: 8px;
    padding: 8px;
    background-color: #0f172a;
    border-radius: 4px;
    font-size: 0.9em;
}
.chatbox .message.user {
    background-color: #334155 !important;
}
.chatbox .message.bot {
    background-color: #1e293b !important;
}
code {
    background-color: #0f172a;
    padding: 2px 4px;
    border-radius: 4px;
}
pre {
    background-color: #0f172a;
    padding: 10px;
    border-radius: 4px;
    overflow-x: auto;
}
"""

# Kh·ªüi t·∫°o c√°c ƒë·ªëi t∆∞·ª£ng query
def get_document_query():
    return DocumentQuery(
        persist_directory=PERSIST_DIRECTORY,
        lm_studio_url=LM_STUDIO_URL,
        model_name=MODEL_NAME
    )

def get_database_query():
    return DatabaseQuery(
        host=MYSQL_HOST,
        user=MYSQL_USER,
        password=MYSQL_PASSWORD,
        port=MYSQL_PORT,
        database=MYSQL_DATABASE,
        lm_studio_url=LM_STUDIO_URL,
        model_name=MODEL_NAME
    )

def get_hybrid_query():
    return HybridQuery(
        lm_studio_url=LM_STUDIO_URL,
        model_name=MODEL_NAME,
        persist_directory=PERSIST_DIRECTORY,
        mysql_host=MYSQL_HOST,
        mysql_user=MYSQL_USER,
        mysql_password=MYSQL_PASSWORD,
        mysql_port=MYSQL_PORT,
        mysql_database=MYSQL_DATABASE
    )

# Ki·ªÉm tra k·∫øt n·ªëi ƒë·∫øn LM Studio API
def check_connection():
    try:
        response = requests.get(f"{LM_STUDIO_URL}/v1/models", timeout=5)
        if response.status_code == 200:
            return True, "ƒê√£ k·∫øt n·ªëi th√†nh c√¥ng ƒë·∫øn LM Studio API"
        else:
            return False, f"Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn LM Studio API. M√£ l·ªói: {response.status_code}"
    except Exception as e:
        return False, f"L·ªói k·∫øt n·ªëi ƒë·∫øn LM Studio API: {str(e)}"

# Ki·ªÉm tra k·∫øt n·ªëi ƒë·∫øn MySQL
def check_mysql_connection():
    try:
        import mysql.connector
        conn = mysql.connector.connect(
            host=MYSQL_HOST,
            user=MYSQL_USER,
            password=MYSQL_PASSWORD,
            port=MYSQL_PORT,
            database=MYSQL_DATABASE,
            connection_timeout=5
        )
        conn.close()
        return True, "ƒê√£ k·∫øt n·ªëi th√†nh c√¥ng ƒë·∫øn MySQL"
    except Exception as e:
        return False, f"L·ªói k·∫øt n·ªëi ƒë·∫øn MySQL: {str(e)}"

# ƒê√°nh gi√° ki·∫øn th·ª©c c·ªßa model
def evaluate_model_knowledge(question):
    """
    ƒê√°nh gi√° xem model c√≥ ƒë·ªß ki·∫øn th·ª©c ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi kh√¥ng
    
    Args:
        question: C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
            
    Returns:
        Tuple[bool, Optional[str]]: (c√≥_th·ªÉ_tr·∫£_l·ªùi, c√¢u_tr·∫£_l·ªùi)
    """
    logger.info(f"ƒê√°nh gi√° ki·∫øn th·ª©c model cho c√¢u h·ªèi: '{question}'")
    
    url = f"{LM_STUDIO_URL}/v1/chat/completions"
    
    system_message = """B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh. 
Tr∆∞·ªõc khi t√¥i truy v·∫•n database ho·∫∑c t√¨m ki·∫øm trong t√†i li·ªáu, h√£y ƒë√°nh gi√° xem b·∫°n c√≥ ki·∫øn th·ª©c ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y kh√¥ng.
N·∫øu b·∫°n bi·∫øt c√¢u tr·∫£ l·ªùi, h√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† ch√≠nh x√°c.
N·∫øu b·∫°n kh√¥ng ch·∫Øc ch·∫Øn ho·∫∑c kh√¥ng bi·∫øt, h√£y ch·ªâ tr·∫£ l·ªùi: "T√îI C·∫¶N TRA C·ª®U TH√äM"
ƒê·ª´ng ƒëo√°n m√≤. N·∫øu kh√¥ng ch·∫Øc ch·∫Øn 100%, h√£y n√≥i b·∫°n c·∫ßn tra c·ª©u th√™m."""
    
    try:
        payload = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": system_message},
                {"role": "user", "content": question}
            ],
            "max_tokens": 1000,
            "temperature": 0.3,
            "stream": False
        }
        
        headers = {
            "Content-Type": "application/json"
        }
        
        response = requests.post(url, json=payload, headers=headers, timeout=30)
        response.raise_for_status()
        result = response.json()
        answer = result.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
        
        # Ki·ªÉm tra xem model c√≥ c·∫ßn tra c·ª©u th√™m kh√¥ng
        needs_lookup = "T√îI C·∫¶N TRA C·ª®U TH√äM" in answer.upper() or any(phrase in answer.lower() for phrase in [
            "t√¥i c·∫ßn tra c·ª©u", "kh√¥ng c√≥ th√¥ng tin", "kh√¥ng bi·∫øt", "kh√¥ng ch·∫Øc ch·∫Øn",
            "kh√¥ng th·ªÉ tr·∫£ l·ªùi", "kh√¥ng ƒë·ªß th√¥ng tin", "c·∫ßn t√¨m hi·ªÉu th√™m"
        ])
        
        if not needs_lookup:
            return True, answer
        else:
            return False, None
    except Exception as e:
        logger.error(f"L·ªói khi ƒë√°nh gi√° ki·∫øn th·ª©c model: {e}")
        return False, None

# H√†m x·ª≠ l√Ω truy v·∫•n
def process_query(message, history, mode, top_k_value, progress=gr.Progress()):
    if not message:
        return "", history
    
    # Hi·ªÉn th·ªã th√¥ng b√°o ƒëang x·ª≠ l√Ω
    processing_message = "‚è≥ ƒêang x·ª≠ l√Ω..."
    yield "", history + [[message, processing_message]]
    
    # C·∫≠p nh·∫≠t gi√° tr·ªã TOP_K
    top_k = int(top_k_value)
    
    try:
        start_time = time.time()
        
        # N·∫øu ch·∫ø ƒë·ªô l√† "auto", h·ªèi model tr∆∞·ªõc
        if mode == "auto":
            model_can_answer, model_answer = evaluate_model_knowledge(message)
            
            if model_can_answer:
                elapsed_time = time.time() - start_time
                response = f"{model_answer}\n\n<div style='font-size: 0.8em; color: gray; text-align: right; margin-top: 10px;'>üß† Tr·∫£ l·ªùi t·ª´ ki·∫øn th·ª©c ƒë√£ hu·∫•n luy·ªán | ‚è±Ô∏è {elapsed_time:.2f} gi√¢y</div>"
                yield "", history + [[message, response]]
                return
            else:
                # N·∫øu model kh√¥ng bi·∫øt, chuy·ªÉn sang ch·∫ø ƒë·ªô hybrid
                mode = "hybrid"
        
        # X·ª≠ l√Ω theo c√°c ch·∫ø ƒë·ªô kh√°c nhau
        if mode == "document":
            yield "", history + [[message, "‚è≥ ƒêang t√¨m ki·∫øm th√¥ng tin..."]]
            doc_query = get_document_query()
            result = doc_query.query(message, top_k=top_k)
            
            response = f"{result['answer']}\n\n"
            
            # Th√™m th√¥ng tin v·ªÅ ngu·ªìn
            response += "<div class='source-info'>"
            response += "<b>üìö Ngu·ªìn t√†i li·ªáu:</b><br>"
            for i, source in enumerate(result['sources'], 1):
                response += f"  {i}. {source}<br>"
            response += "</div>"
                
        elif mode == "database":
            yield "", history + [[message, "‚è≥ ƒêang truy v·∫•n c∆° s·ªü d·ªØ li·ªáu..."]]
            db_query = get_database_query()
            result = db_query.query(message)
            
            if result["success"]:
                response = f"<div class='query-info'>"
                response += f"<b>üîç Truy v·∫•n SQL:</b> <code>{result['sql_query']}</code>"
                response += "</div>"
                
                if isinstance(result["results"], list):
                    if result.get("formatted_results"):
                        response += f"<pre>{result['formatted_results']}</pre>"
                    else:
                        response += f"<b>S·ªë l∆∞·ª£ng k·∫øt qu·∫£:</b> {len(result['results'])}"
                else:
                    response += f"<b>K·∫øt qu·∫£:</b> {result['results']}"
            else:
                response = f"<span class='status-error'>‚ö†Ô∏è L·ªói: {result['message']}</span>"
                
        else:  # hybrid
            yield "", history + [[message, f"‚è≥ ƒêang ph√¢n t√≠ch v√† x·ª≠ l√Ω..."]]
            hybrid_query = get_hybrid_query()
            
            result = hybrid_query.query(message, top_k=top_k)
            
            response = f"{result['answer']}\n\n"
            
            # Th√™m th√¥ng tin v·ªÅ ngu·ªìn
            response += "<div class='source-info'>"
            response += "<b>üìö Ngu·ªìn th√¥ng tin:</b><br>"
            for i, source in enumerate(result['sources'], 1):
                response += f"  {i}. {source}<br>"
            response += "</div>"
            
            # Th√™m th√¥ng tin v·ªÅ lo·∫°i truy v·∫•n
            response += "<div class='query-info'>"
            response += "<b>üîç Lo·∫°i truy v·∫•n:</b><br>"
            
            # Ki·ªÉm tra n·∫øu ƒë√¢y l√† c√¢u tr·∫£ l·ªùi t·ª´ ki·∫øn th·ª©c c√≥ s·∫µn c·ªßa model
            if result["query_type"].get("model_knowledge", False):
                response += "  - üß† Ki·∫øn th·ª©c c·ªßa model: <span class='status-connected'>C√≥</span><br>"
                response += "  - üíæ Database: Kh√¥ng<br>"
                response += "  - üìÑ T√†i li·ªáu: Kh√¥ng<br>"
            else:
                # Hi·ªÉn th·ªã th√¥ng tin n·∫øu kh√¥ng ph·∫£i t·ª´ ki·∫øn th·ª©c model
                if result["query_type"]["database"]:
                    response += "  - üíæ Database: <span class='status-connected'>C√≥</span><br>"
                    if result.get("sql_query"):
                        response += f"    <code>SQL: {result['sql_query']}</code><br>"
                else:
                    response += "  - üíæ Database: Kh√¥ng<br>"
                
                if result["query_type"]["document"]:
                    response += "  - üìÑ T√†i li·ªáu: <span class='status-connected'>C√≥</span><br>"
                else:
                    response += "  - üìÑ T√†i li·ªáu: Kh√¥ng<br>"
            
            response += "</div>"
        
        # Th√™m th√¥ng tin v·ªÅ th·ªùi gian x·ª≠ l√Ω
        elapsed_time = time.time() - start_time
        response += f"<div style='font-size: 0.8em; color: gray; text-align: right; margin-top: 10px;'>‚è±Ô∏è {elapsed_time:.2f} gi√¢y</div>"
                
    except Exception as e:
        logger.error(f"L·ªói khi x·ª≠ l√Ω truy v·∫•n: {e}", exc_info=True)
        response = f"<span class='status-error'>‚ö†Ô∏è ƒê√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω truy v·∫•n: {str(e)}</span>"
    
    yield "", history + [[message, response]]

# T·∫°o vector database
def create_db(docs_dir, chunk_size, chunk_overlap, progress=gr.Progress()):
    try:
        progress(0, desc="ƒêang kh·ªüi t·∫°o...")
        processor = DocumentProcessor(
            docs_dir=docs_dir,
            chunk_size=int(chunk_size),
            chunk_overlap=int(chunk_overlap),
            persist_directory=PERSIST_DIRECTORY
        )
        
        progress(0.2, desc="ƒêang x·ª≠ l√Ω t√†i li·ªáu...")
        processor.process_all(progress_callback=lambda x: progress(0.2 + 0.7 * x, desc=f"ƒêang x·ª≠ l√Ω: {x*100:.0f}%"))
        
        progress(1.0, desc="Ho√†n th√†nh!")
        return f"‚úÖ ƒê√£ t·∫°o vector database th√†nh c√¥ng t·∫°i {PERSIST_DIRECTORY}"
    except Exception as e:
        logger.error(f"L·ªói khi t·∫°o vector database: {e}", exc_info=True)
        return f"‚ùå L·ªói khi t·∫°o vector database: {str(e)}"

# T·∫°o giao di·ªán Gradio
def create_gradio_interface():
    # Ki·ªÉm tra k·∫øt n·ªëi
    lm_connected, lm_status = check_connection()
    db_connected, db_status = check_mysql_connection()
    
    # ƒê·ªãnh nghƒ©a c√°c chu·ªói HTML cho tr·∫°ng th√°i
    lm_connected_html = '<span class="status-connected">‚úÖ ƒê√£ k·∫øt n·ªëi</span>'
    lm_error_html = '<span class="status-error">‚ùå L·ªói k·∫øt n·ªëi</span>'
    db_connected_html = '<span class="status-connected">‚úÖ ƒê√£ k·∫øt n·ªëi</span>'
    db_error_html = '<span class="status-error">‚ùå L·ªói k·∫øt n·ªëi</span>'
    
    # ƒê·ªãnh nghƒ©a ch·ªß ƒë·ªÅ t·ªëi (Dark theme)
    theme = gr.themes.Soft(
        primary_hue="indigo",
        secondary_hue="slate",
        neutral_hue="slate",
        text_size="sm"
    ).set(
        body_background_fill="#0f172a",  # N·ªÅn t·ªëi
        block_background_fill="#1e293b",  # N·ªÅn block t·ªëi
        block_border_width="1px",
        block_border_color="#334155",
        block_radius="8px",
        button_primary_background_fill="#4f46e5",
        button_primary_background_fill_hover="#4338ca",
        button_primary_text_color="#ffffff",
        button_secondary_background_fill="#334155",
        button_secondary_background_fill_hover="#475569",
        button_secondary_text_color="#e2e8f0"
    )
    
    with gr.Blocks(title="RAG Chatbot", theme=theme, css=DARK_CUSTOM_CSS) as demo:
        gr.Markdown(
            """
            # ü§ñ RAG Chatbot
            ### H·ªá th·ªëng h·ªèi ƒë√°p k·∫øt h·ª£p RAG (Retrieval-Augmented Generation) v√† truy v·∫•n c∆° s·ªü d·ªØ li·ªáu
            """
        )
        
        with gr.Tabs() as tabs:
            with gr.TabItem("üí¨ Chat", id=0):
                with gr.Row():
                    with gr.Column(scale=4):
                        chatbot = gr.Chatbot(
                            show_copy_button=True,
                            height=500,
                            avatar_images=("https://t3.ftcdn.net/jpg/03/94/89/90/360_F_394899054_4TMgw6eiMYUfozaZU3Kgr5e0LdH4ZrsU.jpg", "https://www.shutterstock.com/image-vector/chat-bot-icon-virtual-smart-600nw-2478937553.jpg"),
                            bubble_full_width=False,
                            render_markdown=True,
                            elem_classes="chatbox",
                        )
                        
                        with gr.Row():
                            msg = gr.Textbox(
                                placeholder="Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n t·∫°i ƒë√¢y...",
                                container=False,
                                scale=10,
                                autofocus=True,
                            )
                            submit_btn = gr.Button("üöÄ G·ª≠i", variant="primary", scale=1)
                        
                        with gr.Row():
                            clear_btn = gr.Button("üóëÔ∏è X√≥a l·ªãch s·ª≠", variant="secondary")
                            
                    with gr.Column(scale=1):
                        gr.Markdown("### Ch·∫ø ƒë·ªô truy v·∫•n")
                        mode_selector = gr.Radio(
                            ["auto", "hybrid", "document", "database"],
                            label="Ch·ªçn ch·∫ø ƒë·ªô truy v·∫•n ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa b·∫°n",
                            value="auto",
                            container=True,
                        )
                        
                        gr.Markdown("### S·ªë l∆∞·ª£ng k·∫øt qu·∫£ t√¨m ki·∫øm (top-k)")
                        top_k_slider = gr.Slider(
                            minimum=1,
                            maximum=10,
                            value=TOP_K,
                            step=1,
                            label="S·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ khi t√¨m ki·∫øm t√†i li·ªáu",
                        )
                        
                        with gr.Accordion("‚ÑπÔ∏è Tr·∫°ng th√°i h·ªá th·ªëng", open=True):
                            lm_status_component = gr.Markdown(
                                f"**LM Studio API**: {lm_connected_html if lm_connected else lm_error_html}"
                            )
                            db_status_component = gr.Markdown(
                                f"**MySQL Database**: {db_connected_html if db_connected else db_error_html}"
                            )
                            
                            if not lm_connected or not db_connected:
                                gr.Markdown(
                                    f"**Chi ti·∫øt l·ªói**:\n- LM Studio: {lm_status}\n- MySQL: {db_status}"
                                )
                        
                        with gr.Accordion("üîß Th√¥ng tin h·ªá th·ªëng", open=False):
                            model_info = gr.Markdown(
                                f"""
                                - **Model LLM**: {MODEL_NAME}
                                - **API URL**: {LM_STUDIO_URL}
                                - **Vector DB**: {PERSIST_DIRECTORY}
                                - **Database**: {MYSQL_DATABASE}@{MYSQL_HOST}
                                
                                ### Ch·∫ø ƒë·ªô truy v·∫•n
                                - **Auto**: T·ª± ƒë·ªông quy·∫øt ƒë·ªãnh s·ª≠ d·ª•ng ki·∫øn th·ª©c ƒë∆∞·ª£c hu·∫•n luy·ªán s·∫µn ho·∫∑c truy v·∫•n t√†i nguy√™n
                                - **Hybrid**: K·∫øt h·ª£p c·∫£ RAG v√† truy v·∫•n c∆° s·ªü d·ªØ li·ªáu
                                - **Document**: Ch·ªâ s·ª≠ d·ª•ng RAG
                                - **Database**: Ch·ªâ truy v·∫•n c∆° s·ªü d·ªØ li·ªáu
                                """
                            )
                
            with gr.TabItem("üõ†Ô∏è Qu·∫£n l√Ω", id=1):
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### T·∫°o Vector Database")
                        with gr.Row():
                            docs_dir = gr.Textbox(value="./docs", label="Th∆∞ m·ª•c t√†i li·ªáu")
                        
                        with gr.Row():
                            chunk_size = gr.Number(value=500, label="K√≠ch th∆∞·ªõc ƒëo·∫°n vƒÉn b·∫£n", precision=0)
                            chunk_overlap = gr.Number(value=50, label="ƒê·ªô ch·ªìng l·∫•p", precision=0)
                        
                        create_btn = gr.Button("üî® T·∫°o Vector Database", variant="primary")
                        create_output = gr.Markdown()
                        
                        gr.Markdown("### Ki·ªÉm tra k·∫øt n·ªëi")
                        with gr.Row():
                            check_lm_btn = gr.Button("üîÑ Ki·ªÉm tra LM Studio API", variant="secondary")
                            check_db_btn = gr.Button("üîÑ Ki·ªÉm tra MySQL", variant="secondary")
                        
                        connection_status = gr.Markdown()
                
                with gr.Accordion("üìñ H∆∞·ªõng d·∫´n", open=False):
                    gr.Markdown(
                        """
                        ### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng

                        1. **T·∫°o Vector Database**:
                           - ƒê·∫£m b·∫£o t√†i li·ªáu ƒë√£ ƒë∆∞·ª£c ƒë·∫∑t trong th∆∞ m·ª•c ch·ªâ ƒë·ªãnh
                           - Nh·∫•n n√∫t "T·∫°o Vector Database" ƒë·ªÉ t·∫°o c∆° s·ªü d·ªØ li·ªáu vector t·ª´ t√†i li·ªáu

                        2. **S·ª≠ d·ª•ng chatbot**:
                           - Ch·ªçn ch·∫ø ƒë·ªô truy v·∫•n ph√π h·ª£p (auto, hybrid, document, database)
                           - Nh·∫≠p c√¢u h·ªèi v√† nh·∫•n "G·ª≠i" ho·∫∑c Enter
                           - K·∫øt qu·∫£ s·∫Ω ƒë∆∞·ª£c hi·ªÉn th·ªã trong c·ª≠a s·ªï chat

                        3. **C√°c ch·∫ø ƒë·ªô truy v·∫•n**:
                           - **Auto**: T·ª± ƒë·ªông s·ª≠ d·ª•ng ki·∫øn th·ª©c c√≥ s·∫µn ho·∫∑c truy v·∫•n t√†i nguy√™n khi c·∫ßn
                           - **Hybrid**: K·∫øt h·ª£p c·∫£ RAG v√† truy v·∫•n c∆° s·ªü d·ªØ li·ªáu
                           - **Document**: Ch·ªâ s·ª≠ d·ª•ng RAG ƒë·ªÉ tr·∫£ l·ªùi t·ª´ t√†i li·ªáu
                           - **Database**: Ch·ªâ truy v·∫•n c∆° s·ªü d·ªØ li·ªáu

                        4. **M·∫πo t·ªëi ∆∞u hi·ªáu su·∫•t**:
                           - ƒê·ªëi v·ªõi c√¢u h·ªèi ƒë∆°n gi·∫£n, h·ªá th·ªëng s·∫Ω tr·∫£ l·ªùi ngay l·∫≠p t·ª©c
                           - Ch·ªçn ch·∫ø ƒë·ªô truy v·∫•n ph√π h·ª£p ƒë·ªÉ t·ªëi ∆∞u th·ªùi gian x·ª≠ l√Ω
                           - ƒêi·ªÅu ch·ªânh top-k nh·ªè h∆°n ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω
                        """
                    )
                
        # X·ª≠ l√Ω c√°c s·ª± ki·ªán
        submit_btn.click(
            process_query,
            inputs=[msg, chatbot, mode_selector, top_k_slider],
            outputs=[msg, chatbot],
            api_name="submit",
        )
        
        msg.submit(
            process_query,
            inputs=[msg, chatbot, mode_selector, top_k_slider],
            outputs=[msg, chatbot],
            api_name="submit_enter",
        )
        
        clear_btn.click(
            lambda: (None, []),
            inputs=None,
            outputs=[msg, chatbot],
        )
        
        # X·ª≠ l√Ω s·ª± ki·ªán t·∫°o vector database
        create_btn.click(
            create_db,
            inputs=[docs_dir, chunk_size, chunk_overlap],
            outputs=[create_output],
        )
        
        # X·ª≠ l√Ω s·ª± ki·ªán ki·ªÉm tra k·∫øt n·ªëi
        check_lm_btn.click(
            lambda: check_connection()[1],
            inputs=None,
            outputs=[connection_status],
        )
        
        check_db_btn.click(
            lambda: check_mysql_connection()[1],
            inputs=None,
            outputs=[connection_status],
        )
        
    return demo

if __name__ == "__main__":
    # Ki·ªÉm tra xem vector database ƒë√£ t·ªìn t·∫°i ch∆∞a
    if not os.path.exists(PERSIST_DIRECTORY):
        logger.warning(f"Vector database kh√¥ng t·ªìn t·∫°i t·∫°i {PERSIST_DIRECTORY}. B·∫°n c√≥ th·ªÉ t·∫°o m·ªõi trong tab Qu·∫£n l√Ω.")
    
    # Kh·ªüi ƒë·ªông giao di·ªán Gradio
    demo = create_gradio_interface()
    demo.launch(share=False, server_name="127.0.0.1") 